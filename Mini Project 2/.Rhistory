bestfits <- regsubsets(Price ~ KmDriven + FuelType + Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
summary(bestfits)
model <- lm(Price ~ KmDriven + FuelType + Mileage + Engine + Power + Age + Seats, data = train)
summary(model)
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(vif)
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
#checking for multicollinearity
vif(model)
source("resFigs.R")
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
res.figs(model)
ggpairs(train,
columns = 1:nrow(train),
title = "Plot")
ggpairs(train,
columns = nrow(train),
title = "Plot")
ggpairs(train,
columns = 1:9,
title = "Plot")
durbinWatsonTest(model)
ncv.test(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ KmDriven + FuelType + Mileage + Engine + Power + Age + Seats, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 4)  # Cook's Distance
plot(model, which = 4)
plot(log_model, which = 4)  # Cook's Distance
cooks <- cooks.distance(log_model)
high_cooks <- which(cooks > 1)  # Identify observations with high Cook's Distance
print(high_cooks)
plot(log_model, which = 4)  # Cook's Distance
cooks <- cooks.distance(log_model)
high_cooks <- which(cooks > 1)  # Identify observations with high Cook's Distance
print(high_cooks)
cor(train[, -which(names(train) == "Price")], cooks)  # Exclude Price
leverage <- hatvalues(log_model)
print(leverage[high_cooks])  # View leverage for high Cook's Distance points
vif(log_model)
plot(model, which = 1)  # Residuals vs. Fitted plot
plot(log_model, which = 1)  # Residuals vs. Fitted plot
plot(model , which =1)
crPlots(log_model)  # Generate component + residual plots
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Mileage + Engine + Power + Age + Seats, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
durbinWatsonTest(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ log(KmDriven) + FuelType + log(Mileage) + Engine + Power + Age + Seats, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
crPlots(log_model)  # Generate component + residual plots
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ log(KmDriven) + FuelType + log(Mileage) + Engine + Power + Age + Seats, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
crPlots(log_model)  # Generate component + residual plots
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage  + Power, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ log(KmDriven) + FuelType + log(Mileage) + Engine + Power + Age + Seats, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
crPlots(log_model)  # Generate component + residual plots
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage  + Power  + Seats, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ log(KmDriven) + FuelType + log(Mileage) + Engine + Power + Age + Seats, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
crPlots(log_model)  # Generate component + residual plots
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage  + Power  + Seats, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ log(KmDriven) + FuelType + log(Mileage) + Engine + Power + Age + Seats, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
crPlots(log_model)  # Generate component + residual plots
#log Full Regression Model
log_model <- lm(log(Price) ~ KmDriven + FuelType + Mileage + Power + Seats, data = train)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
summary(bestfits)
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage + Power + Age, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model , which =2)
plot(model , which =1)
#Full Regression Model
model <- lm(Price ~ KmDriven, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ KmDriven + FuelType + Transmission + Mileage + Power + Age, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage + Power + Seats, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
ncvTest(model)
#log Full Regression Model
log_model <- lm(log(Price) ~ KmDriven + FuelType + Transmission + Mileage + Power + Age, data = train)
summary(model)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
crPlots(log_model)  # Generate component + residual plots
library(tidyverse)
library(ggplot2)
library(GGally)
library(dplyr)
library(car)
auto <- read.csv("auto.csv")
#auto_new <- read.csv("auto.csv")
auto <- select(auto, -1)
head(auto, 4)
ggpairs(auto,
columns = 2:6,
title = "Pairwise scatterplot for each variable ")
correlation_matrix <- cor(auto, method = 'pearson')
correlation_matrix
lm <- lm(mpg ~ hp + wt + acc + disp + cyl, data = auto)
summary(lm)
model <- lm(mpg ~ hp + wt + cyl, data = auto)
summary(model)
qqPlot(model, main = "Q-Q Plot")  # Checking for normality
ncvTest(model)  # Checking for homoscedasticity
durbinWatsonTest(model)  # Checking for independence of residuals
fitted_values <- fitted(model)
plot(fitted_values, residuals(model),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residuals Plot")
log_model <- lm(log(mpg) ~ hp + wt + cyl, data = auto)
summary(model)
qqPlot(log_model, main = "QQ-plot")
ncvTest(log_model)
log_fitted_values <- fitted(log_model)
plot(fitted_values, residuals(log_model),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residuals Plot")
durbinWatsonTest(log_model)
residuals <- residuals(log_model)
residual_plot <- plot(log_fitted_values, residuals(log_model),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residuals",
col = c("red","blue","green")[as.factor(auto$cyl)])
legend("bottomleft", legend = c("4-Cylinders",
"6-Cylinders",
"8-Cylinders"),
col = c("red","blue","green"),
pch = 19)
vif(log_model)
log_model_2 <- lm(log(mpg) ~ wt + cyl, data = auto)
summary(model)
qqPlot(log_model_2) #Normality
ncvTest(log_model_2)#Constant variance
log_fitted_values_2 <- fitted(log_model_2)
plot(fitted_values, residuals(log_model_2),
xlab = "Fitted Values",
ylab = "Residuals",
main = "Residuals Plot")
durbinWatsonTest(log_model_2)
interaction_model <- lm(log(mpg) ~ hp * wt + cyl, data = auto)
anova(log_model_2, interaction_model)
summary(interaction_model)
plot(interaction_model , which  = 1)
#one-tailed test
t_critical <- qt(0.95, df = 195)
t_critical
# Predicting log(mpg) with confidence interval
estimate <- predict(
interaction_model,
newdata = data.frame(wt = 3, cyl = 4, hp = mean(auto$hp[auto$cyl == 4])),
interval = "confidence"
)
#converting to mpg values
mpg_estimate <- exp(estimate)
mpg_estimate
# Calculating mean horsepower for 8-cylinder cars
mean_hp <- mean(auto$hp[auto$cyl == 8])
# Predicting log(mpg) with confidence interval
f150_estimates <- predict(
interaction_model,
newdata = data.frame(wt = 4.5, cyl = 8, hp = mean_hp),
interval = "confidence"
)
# converting to mpg values
f150_mpg <- exp(f150_estimates)
# Display the results
f150_mpg
log_model <- lm(LogPrice ~ Mileage * FuelType + Power * Transmission + Engine * Age, data = train_data)
log_model <- lm(LogPrice ~ Mileage * FuelType + Power * Transmission + Engine * Age, data = train)
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")
used_cars <- read.csv("usedCars.csv")
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for transmission, 1 = Automatic, 0 = Manua
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)
#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)
#training/testing split
set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)
train <- used_cars[train_ind, ]
test <- used_cars[-train_ind, ]
#correlation matrix of every variable
correlation_matrix <- cor(train, method = "pearson")
correlation_matrix
correlation_matrix["Price", ]
#optimal set of predictor variables
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission +  Mileage + Engine + Power + Age + Seats,
data = train, nbest = 2)
plot(bestfits, scale = "adjr2")
#Full Regression Model
model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage + Power + Seats, data = train)
summary(model)
#checking for multicollinearity
vif(model)
res.figs(model)
plot(model , which =1)
plot(model, which = 4)
#Checking for independence
durbinWatsonTest(model)
ncvTest(model)
log_model <- lm(LogPrice ~ Mileage * FuelType + Power * Transmission + Engine * Age, data = train)
log_model <- lm(Log(Price) ~ Mileage * FuelType + Power * Transmission + Engine * Age, data = train)
log_model <- lm(log(Price) ~ Mileage * FuelType + Power * Transmission + Engine * Age, data = train)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
# Build a linear regression model with Mileage × Age and FuelType × Engine interactions
model <- lm(log(Price) ~ Mileage * Age + FuelType * Engine + Transmission + Power + Seats,
data = traoin)
# Build a linear regression model with Mileage × Age and FuelType × Engine interactions
model <- lm(log(Price) ~ Mileage * Age + FuelType * Engine + Transmission + Power + Seats,
data = train)
ncvTest(log_model)
plot(log_model, which = 1)  # Residuals vs. Fitted plot
