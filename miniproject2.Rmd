---
title: "MiniProj2"
output: html_document
date: "2024-11-28"
---


```{r}
rm(list = ls())
library(GGally)
library(dplyr)
library(corrplot)
library(leaps)
library("RColorBrewer")
library(car)
source("resFigs.R")

```


```{r}
used_cars <- read.csv("usedCars.csv")


#Price difference between Automatic vs manual
aggregate(Price ~ Transmission, data = used_cars, FUN = mean)

#Price difference between Automatic vs manual
aggregate(Price ~ FuelType, data = used_cars, FUN = mean)
```
```{r}
# Bar plot for Transmission
ggplot(used_cars, aes(x = Transmission, y = Price)) +
  stat_summary(fun = mean, geom = "bar", fill = "lightblue", color = "black") +
  labs(title = "Average Price by Transmission", x = "Transmission", y = "Average Price (in ₹)") +
  theme_minimal()

```
```{r}
ggplot(used_cars,
       aes(x = FuelType,
           y = Price)) +
  stat_summary(fun = "mean", geom = "bar", fill = "orange", color = "black") +
  labs(title = "Average Price by FuelType", x = "Transmission", y = "Average Price (in ₹)") +
  theme_minimal()
```

```{r}
#indicator variable for transmission, 1 = Automatic, 0 = Manual
used_cars$Transmission <- ifelse(used_cars$Transmission == "Automatic", 1,0)
#indicator variable for FuelType, 1 = Petrol, 0 = Diesel
used_cars$FuelType <- ifelse(used_cars$FuelType == "Petrol", 1, 0)

#Dropping carID variable
used_cars <- select(used_cars, -1)
head(used_cars)

```


```{r}
#training/testing split

set.seed(3265)
train_ind <- sample(1:nrow(used_cars), floor(0.8 *nrow(used_cars)))
set.seed(NULL)

train <- used_cars[train_ind, ] 
test <- used_cars[-train_ind, ]


```

# Checking which vandidate predictors wil cause high influential points



```{r}
#correlation matrix of every variable

correlation_matrix <- cor(train, method = "pearson")
correlation_matrix


```
```{r}
cor <- cor(train)
corrplot(cor, type = "upper", order = "hclust", tl.col = "black", tl.srt = 45)
```

```{r}
correlation_matrix["Price", ]

```
```{r}
# Visualize relationships
ggplot(train, aes(x = Price, y = KmDriven)) + geom_point() + geom_smooth(method = "lm")
ggplot(train, aes(x = Price, y = Mileage)) + geom_point() + geom_smooth(method = "lm")
ggplot(train, aes(x = Price, y= Power)) + geom_point() + geom_smooth(method=  "lm")
```
```{r}
library(leaps); library("RColorBrewer")
bestfits <- regsubsets(Price ~ KmDriven + FuelType + Transmission + Mileage + Engine + Power + Age + Seats ,
                       data = train, nbest = 2)
plot(bestfits, scale="adjr2")
```
Based on the bestfits, we remove the following candidate predictors:
Engine
Age
Seats

# 5% TEST
```{r}
#used_cars$Price <- log(used_cars$Price)
#Full Regression Model
full_model <- lm(Price ~ KmDriven + FuelType + Transmission + Mileage + Engine + Power + Age + Seats , data = train)


summary(full_model)
res.figs(full_model)
```
Hypothesis:

Null: $H_0$: $\beta_i$ = 0
Alternate: $H_1$: $\beta_i$ != 0

Based on the test we reject Null Hyothesis if p-value < 0.05; therefore, we remove the following candidate predictors at 5%:
Engine
Age
Seats




# Forward selection
```{r}
#null Model
null_model <- lm(Price ~ 1, data = train)

forward_model <- step(null_model, scope = formula(full_model), direction = "forward")
```

All three tests suggest that we remove the following predictors:
Engine
Age
Seats

```{r}
res.figs(forward_model)
```

The model appears to be homoscedastic because the plot is not forming a funnel shape and the residuals appear to show a slight increase in spread as the fitted values grow larger.

```{r}
anova(full_model, forward_model)
```
Null: $H_0$ : $\beta_Engine$ =  $\beta_Age$ =  $\beta_Seats$ = 0
The additional predictors in the model do not significantly improve the models performance.

Alternate: $H_1$: $\beta_Engine$ =  $\beta_Age$ =  $\beta_Seats$ != 0
The additional predictors significantly improve the models performance.

Since p-value 0.4542 > 0.05, we fail to reject null hypothesis this means that difference in performance between full_model and reduced_model is not statistically significant.

# Interpreting the models 

```{r}
summary(forward_model)
```

Power	
6647.41	Positive	For every unit increase in Power, the sale price increases by ₹6,647.41.
Transmission	
560067.08
560067.08	Positive	Cars with the alternate transmission (e.g., automatic vs manual) are worth ₹560,067 more.

FuelType	
−433050.15	Negative	Cars with a different fuel type (e.g., diesel vs petrol) are worth ₹433,050 less.


KmDriven	
−15.42	Negative	For every additional kilometer driven, the sale price decreases by ₹15.42.
Mileage	
−
49203.76
−49203.76	Negative	For every unit increase in mileage (fuel consumption), the sale price decreases by ₹49,203.76.

```{r}
vif(forward_model)
```

```{r}
# Extract coefficients
coeff <- coef(forward_model)
#Removing intercept
coeff <- coeff[-1]

#Plotting
barplot(coeff,
        xlab = "Predictors",
        ylab = "Coefficient Estimate",
        main = "Effect of Predictors on sale price",
        col = ifelse(coeff > 0, "steelblue", "red"))
```
# Prediciting the Prices 
```{r}
test$Predicted_price <- predict(forward_model, newdata = test)
test$Residuals <- test$Price - test$Predicted_price
summary(test$Residuals)
```

```{r}

# Residual Plot with Lines
ggplot(test, aes(x = Predicted_price, y = Residuals)) +
  geom_point(color = "blue") +  # Scatter plot of residuals
  geom_segment(aes(x = Predicted_price, 
                   xend = Predicted_price, 
                   y = 0, 
                   yend = Residuals), 
               color = "red") +  # Residual lines
  geom_hline(yintercept = 0, color = "black") +  # Zero residual line
  ggtitle("Residuals with Residual Lines") +
  xlab("Predicted Sale Price") +
  ylab("Residuals")

```




As predicted sale prices increase, the residuals become larger (both positive and negative).
This indicates that the model struggles to predict accurately for expensive cars, possibly due to:
Non-linearity in the data.
Lack of features to explain high-priced cars.



```{r}
MAE <- mean(abs(test$Residuals))
RMSE <- sqrt(mean(test$Residuals^2))
MAPE <- mean(abs(test$Residuals / test$Price)) * 100

cat(" MAE:", MAE, "\n", "RMSE:", RMSE, "\n", "MAPE:", MAPE, "\n")

```
The model's predictions deviate by 25.21% from the actual car prices.


# Testing on newCars data

```{r}
newCars <- read.csv("newCars.csv")
newCarResults <- read.csv("newCarResults.csv")

#indicator variable for transmission, 1 = Automatic, 0 = Manual
newCars$Transmission <- ifelse(newCars$Transmission == "Automatic", 1,0)
#indicator variable for FuelType, 1 = Petrol, 0 = Diesel
newCars$FuelType <- ifelse(newCars$FuelType == "Petrol", 1, 0)
head(newCars, 4 )
```

```{r}
newCars$PredPrice <- predict(forward_model, newdata = newCars)
# Calculate prediction intervals
prediction_intervals <- predict(forward_model, newdata = newCars, interval = "prediction")


# Add intervals to the newCars dataset
newCars$LowPrice <- prediction_intervals[, "lwr"]  # Lower bound of interval
newCars$HighPrice <- prediction_intervals[, "upr"] # Upper bound of interval


# Select only the required columns
newCarResults <- newCars[, c("carID", "PredPrice", "LowPrice", "HighPrice")]

# Save to newCarResults.csv
write.csv(newCarResults, "newCarResults.csv", row.names = FALSE)

```

